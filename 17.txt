# ------------------------------------------------------------
# STEP a: Import Required Libraries
# ------------------------------------------------------------
import pandas as pd
import matplotlib.pyplot as plt
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# ------------------------------------------------------------
# STEP b: Load Dataset
# ------------------------------------------------------------
df = pd.read_csv(r"C:\Users\Vedant123\Downloads\datasets\Order3.csv")
print("âœ… Dataset Loaded Successfully!")
print(df.head())

# ------------------------------------------------------------
# STEP c: Data Pre-processing
# ------------------------------------------------------------
# Drop missing values (if any)
df.dropna(inplace=True)

# Display column names for clarity
print("\nColumns in dataset:", df.columns.tolist())

# We assume one column (like 'Transaction' or 'Member_number') identifies transactions,
# and another column (like 'itemDescription') holds the purchased items.

# Adjust column names as needed based on your CSV file (example below):
# df.rename(columns={'TransactionNo': 'Transaction', 'Itemname': 'itemDescription'}, inplace=True)

# Group items per transaction (each transaction = list of items)
transactions = df.groupby(df.columns[0])[df.columns[-1]].apply(list).values.tolist()

print(f"\nâœ… Total Transactions: {len(transactions)}")
print("Sample Transaction:\n", transactions[0])

# ------------------------------------------------------------
# STEP d: Encode Transactions (One-Hot Encoding)
# ------------------------------------------------------------
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_encoded = pd.DataFrame(te_ary, columns=te.columns_)

print("\nâœ… One-Hot Encoded Transaction Data (first 5 rows):")
print(df_encoded.head())

# ------------------------------------------------------------
# STEP e: Apply Apriori Algorithm
# ------------------------------------------------------------
# Find frequent itemsets with minimum support of 1%
frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)
frequent_itemsets.sort_values(by='support', ascending=False, inplace=True)

print("\nðŸ”¹ Frequent Itemsets (Top 10):")
print(frequent_itemsets.head(10))

# ------------------------------------------------------------
# STEP f: Generate Association Rules
# ------------------------------------------------------------
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)
rules.sort_values(by='lift', ascending=False, inplace=True)

print("\nðŸ”¹ Association Rules (Top 10):")
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))

# ------------------------------------------------------------
# STEP g: Visualization - Support vs Confidence
# ------------------------------------------------------------
plt.figure(figsize=(8,5))
plt.scatter(rules['support'], rules['confidence'], c=rules['lift'], cmap='viridis', s=80)
plt.colorbar(label='Lift')
plt.xlabel('Support')
plt.ylabel('Confidence')
plt.title('Apriori Algorithm - Support vs Confidence')
plt.show()

# ------------------------------------------------------------
# STEP h: Visualization - Top 10 Frequent Itemsets
# ------------------------------------------------------------
plt.figure(figsize=(10,5))
plt.bar(frequent_itemsets['itemsets'].astype(str).head(10), frequent_itemsets['support'].head(10), color='orange')
plt.xticks(rotation=45, ha='right')
plt.xlabel('Itemsets')
plt.ylabel('Support')
plt.title('Top 10 Frequent Itemsets (Bakery Orders)')
plt.show()
